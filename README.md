# CRAFT

# _テキスト検出のための文字領域認識_

Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee∗ 
Clova AI Research, NAVER Corp.

## 要旨
ニューラルネットワークに基づくシーンテキスト検知方法が最近浮上し、有望な結果を得ました。
従来の方法では、文字間の関係性と文字間の親和性を検出するために新しいシーン検知方法を提案します。
個々の文字レベルの注釈の欠如を克服するために、学習した中間モデルで獲得した実像のための文字レベルの注釈と文字レベルのグランドトゥルースの両方を利用します。
文字間の親和性を推定するために、ネットワークは新規提案された親和性のための表現で訓練されます。
自然画像の中の高カーブテキストを含む6つのベンチマークの拡張実験と、文字レベル検出が最先端検知器を大幅に上回ることを実証します。
その結果、提案手法は恣意的、カーブ、変形文などの複雑なシーン画像の検知において、高柔軟性を保証します。

## 1.序章
瞬間翻訳、画像検索、地中検索、盲点検など、数多くのアプリケーションがあり、コンピュータ視野で注目されています。
最近、ディープラーニングをベースにしたテキストデテクター(8、40、40、21、10、12、24、25、32、26)は、ワードライブボックスのローカライズを主にしています。
しかし、単一バウンディングボックスでは検知しにくい曲線、変形、極端に長いテキストなど、難しい場合には苦しみます。
代わりに、文字をボトムアップで結びつけることでテキストにチャレンジすると、文字レベル認識は多くのメリットがあります。
既存のテキストデータセットの大部分は文字レベルの注釈ではなく、文字レベルのグランドトゥルースを得るために必要な作業は高すぎます。
本稿では個々の文字領域をローカライズし、検知文字をテキスト例にリンクするテキスト検知器を提案します。
文字領域認知のためのCRAFTと呼ばれる枠文字領域認知のために、文字領域スコアと親和スコアを出して複雑なニューラルネットワークで設計します。
領域スコアはイメージの個々の文字をローカライズし、親和スコアは各文字を一例にまとめて使います。
文字レベルの注釈の欠如を補うために、既存のリアルワードレベルデータセットの文字レベルグランドトゥルースを見積もる弱教師あり学習枠を提案します。  
![Figure1](https://github.com/inouetaka/CRAFT/blob/master/images/figure1.png)

## 2.関連作業
ディープラーニング登場前のシーン検知の主なトレンドはボトムアップであり、手作りの機能が主に使われていた(MSER[27]やSWT[5])。最近はSSD[20]、Faster R-CNN[30]、FCN[23]のような人気物体検知/セグメンテーション法を採用してディープラーニングベースのテキスト検知器が提案されている。
### 回帰ベースのテキスト探知器
人気物検知器を用いた様々なテキストレグレージョン適応が提案されています。一般的な物とは違い、様々なアスペクト比で不規則な形でテキストが提示されることが多いです。この問題に対処するために、テキストボックス[18] 変形コンカーネルとアンカーボックスは様々なテキスト形状を効果的に捕捉するために改良されています。DMPNet[22] は四角いスライディングウィンドウを取り入れることで問題のさらなる減らしに努めました。最近では、回転感覚回帰検知器(RSDD) [19] ボレーションフィルターを積極的に回転させることで不変の特徴を最大限に活用することが提案されました。

### セグメンテーションベースのテキスト探知機
もう一つの共通アプローチは、ピクセルレベルでテキスト領域を探求するセグメンテーションの手法です。マルチスケールFCN[7]、ホリスティック予測[37]、PixelLink[4]などの単語境界領域を推定してテキストを検出するアプローチはセグメンテーションをベースに提案しました。SSTD[8]は特徴レベルの背景干渉を減らして関連領域を強化するアテンションメカニズムを用いて回帰とセグメンテーションの両方の恩恵を受けようとしました。最近、テキストスネーク[24]は幾何学属性と共にテキスト領域とセンターラインを予測してテキストインスタンスを検知することを提案しました。

### エンドツーエンドのテキスト探知機
認識結果を活用して検知精度を高めるために、一斉検知と認識モジュールを同時に教育します。FOTS[21]とEAA[10]の一般検知と認識方法を組み合わせて、エンドツーエンドでトレーニングします。マスクテキストスポッター[25]は、統一モデルを利用して認識作業を意味論セグメンテーション問題として扱いました。認識モジュールでのトレーニングは、テキスト感知器がテキストのような背景の雑然としたものになるのを助けるのは明白です。
ほとんどの方法では単語でテキストを検知しますが、検知のために単語にするのは些細なことではありません。なぜなら単語は意味、空間、色など様々な基準で分けることができるからです。また、単語のセグメンテーションの境界は厳密に定義できないので、単語のセグメント自体は明確な意味合いがありません。この単語の注釈の曖昧さは回帰とセグメンテーションの両方のアプローチのためのグランドトゥルースの意味を希釈します。

### 文字レベルのテキスト探知機
チャンら[39]は、MSERで蒸留したテキストブロック候補で文字レベル感知器を提案しました。[27]では個々の文字を識別するためにMSERを使用することで、対照シーンなどの特定の状況下での感知力強さを制限しています。　　
ヤオら[37]は、文字領域の地図と、文字レベルの注釈が必要な連動方向の予測マップを使いました。明示文字レベル予測の代わりに、セグリンク[32]はテキストグリッド(一部テキストセグメント)を狩り、これらのセグメントを連想させて追加リンク予測を行います。マスクテキストスポッター[25]は文字レベル確率マップを予測しますが、個々の文字を見分ける代わりにテキスト記録に使用しました。
弱教師あり学習の枠組みで文字レベル感知器の学習を行うWordSup[12]の発想にインスパイアされた作品です。しかし、Wordsupの欠点は、文字の表現が長方形のアンカーで形成されていることで、カメラの見方によって誘導される文字のパース変形に弱いことです。また、バックボーン構造の性能(SSDを使用し、アンカーボックスの数と大きさで制限されていること)に縛られています。  
![Figure2](https://github.com/inouetaka/CRAFT/blob/master/images/figure2.png)

## 3.方法論
主な目的は、個々の文字を自然画像で正確にローカライズすることです。そのために、文字領域や文字間の親和性を予測する深層ネットワークを学習しています。公的文字レベルのデータセットがないので、弱教師あり学習を受けています。

### 3.1.構成
バッチノーマライゼーションでVGG-16[34]をベースとした全結合ネットワーク構造をバックボーンに採用しました。低レベルの特徴を集約する点でU-net[31]に似たデコーディング部分はスキップ接続があります。最終出力はスコアマップとして2チャンネルあります。リージョンスコアと親和スコアです。ネットワーク構造は図2で計画的に図示しています。

### 3.2.学習  

### 3.2.1 グランドトゥルースラベル生成
学習イメージ毎に、領域スコアのグランドトゥルースラベルと文字境界ボックスの親和スコアを作成します。領域スコアは与えられたピクセルが文字の中心である確率を表し、親和スコアは隣接文字の間隔のセンター確率を表します。
各ピクセルに個別にラベルを貼る連続セグメンテーションマップと違い、ガウシアンヒートマップで文字センターの確率をエンコードします。グランドトゥルースにこだわらない領域に対処する際の高柔軟性のため、ポーズ推定ワーク[1,29]などの他の用途で使用しています。領域スコアと親和スコアの両方を習得するためにヒートマップ表現を使用します。
図3は合成イメージのラベル生成パイプラインを要約します。境界ボックス内の各ピクセルの直接的な正規分布の値を計算するのはとても時間がかかります。イメージ上の文字の境界ボックスは概ね歪んでいるので、領域スコアと親和スコアの両方のグランドトゥルースをおおよそに生成します。1)2次元等方性ガウス図を準備します。2)ガウシアン図と各文字ボックスの間のパースチェンジ。3)ウォープガウシアンマップをボックスエリアにします。
親和スコアの地盤部分は、図3のように隣接文字ボックスで親和性ボックスを定義します。それぞれのキャラクターボックスの対角に対角になるように対角線を描くことで、上と下の三角形と呼ぶ三角形を生成します。次に、隣接する文字ボックスペアでは、上と下の三角形の中心を箱の隅に設けて親和性ボックスを作ります。
グランドトゥルースの定義を提案すると、小さい受信場ではあるが、長さの大きい文章や長さのある文章を十分に検知できるようになっている。一方、回帰ボックスのような従来のアプローチは、そのような場合には受容場が大きい必要がある。我々の文字レベル検知は、文章例全体ではなく、入り組んだフィルターとインターキャラクターのみに集中することができるようになっている。
![Figure3](https://github.com/inouetaka/CRAFT/blob/master/images/figure3.png)

### 3.2.2 弱教師付き学習
合成データセットとは異なり、データセットのリアル画像は通常単語レベルの注釈があります。ここでは、図4に要約したように、統計的に各単語レベルの注釈から、弱い教師で文字ボックスを作ります。単語レベルの注釈でリアルイメージを設けた場合、学習中間モデルは、クロップされた単語イメージの文字領域スコアを予測して、文字レベルの境界ボックスを作ります。中間モデルの予測の信頼性を反映するため、トレーニング中の重さ学習に使われるグランドトゥルース文字の数で割った検出文字の数に比例して、各単語ボックス上の信頼図の値を算出します。
![Figure4](https://github.com/inouetaka/CRAFT/blob/master/images/figure4.png)  

図6は文字の割り振りの全工程を示しています。まず、オリジナルイメージから言葉レベルのイメージを描きます。第2に、最新のトレーニングを受けたモデルは領域のスコアを予測します。第3に、文字のバウンディングボックスを形成するために使われるキャラクタ領域の分割に使います。最後に、文字ボックスのコーディネートは、刈り込み段階から変換することでオリジナルイメージコーディネートに戻します。領域のスコアは擬似グランドトゥルース(pseudoGTs)と親和スコアは、図3の手順で作成できます。獲得した四角い文字レベルのバウンディングボックスで行います。
![figure6](https://github.com/inouetaka/CRAFT/blob/master/images/figure6.png)  

モデルのトレーニングは不完全な擬似GTでトレーニングします。もしモデルが不正確な領域スコアでトレーニングされれば、文字領域で出力がぼやけてしまうかもしれません。これを防ぐために、モデルで発生する擬似GTの品質を測ります。幸いにも文字の長さである注釈には非常に強いキューがあります。ほとんどのデータセットでは単語の書き継ぎが設けられており、単語の長さは偽GTの信頼度を評価するのに使えます。　　
　　
  
トレーニングデータの単語レベル注釈サンプルwは、R(w)とl(l)をそれぞれ標本wの境界領域と語長にします。字割り工程で、推定文字の境界ボックスと対応文字の長さlc(w)を入手できます。そして標本wの確信スコアsconf(w)を計算します。
![SconF(w)](https://github.com/inouetaka/CRAFT/blob/master/images/SconF(w).png)


画像のピクセルに関するスキャンは、次のように表現されています。  
![Sc(p)](https://github.com/inouetaka/CRAFT/blob/master/images/Sc(p).png)
  
ここでPとは領域R(w)のピクセルを意味し、目標Lは次のように定義します。
![L](https://github.com/inouetaka/CRAFT/blob/master/images/L.png)

Sr∗(p)とSa∗(p)が擬地グランドトゥルーススコアを表し、親和図がそれぞれSr(p)とSa(p)が予測領域スコアと親和スコアを表します。合成データで訓練すると真のグランドトゥルースが得られるのでSc(p)は1に設定します。

トレーニングを行うことで、CRAFTモデルは文字をより正確に予測でき、sconf(w)も徐々に増加していきます。図5はトレーニング中の文字領域のスコアマップです。トレーニングの初期段階では、自然画像で見慣れないテキストは領域のスコアが比較的低いです。モデルは不規則フォントやSynthTextデータセットとは異なるデータ分布を持つ合成テキストなどの新しいテキストの外観を学習します。

確信得点sconf(w)が0.5以下ならモデルのトレーニング時に悪影響があるので推定文字境界ボックスは無視してください。この場合、個々の文字の幅は一定であると仮定し、l(w)の文字数で単純にRegion(w)を分けるだけで文字レベルの予測を計算します。そしてsconf(w)を0.5にして目に見えないテキストの外観を学習します。
![figure5](https://github.com/inouetaka/CRAFT/blob/master/images/figure5.png)

## 推論
推論段階では、最終出力は単語ボックスや文字ボックス等様々な形で格納でき、さらに多角的に格納できます。ICDAR等のデータセットは、評価プロトコルは単語レベルの交差点U(IoU)なので、予測されたSrとSaからの単語境界ボックスの作り方を、シンプルかつ効果的な後処理段階で記述します。

境界ボックス発見後の処理としては、1. Sr(p)>τrまたはSa(p)>τaで画像を覆うバイナリマップMを1に初期化し、τrを領域閾値、τaをアフィニティ閾値とする。2. Mで接続されたコンポーネントラベル(CCL)を行う。3. QuadBoxは、各ラベルに対応する接続されたコンポーネントを囲む最小の領域で回転した矩形を検出することにより得られる、というように要約され、OpenCVが提供するコネクテッドコンポーネントやminAreactのような機能を適用することができる。

なお、CRAFTの利点は、非最大サプレッション(NMS)のように、それ以上の後処理方法を必要としないことである。我々は、CCLによって分離されたワード領域のイメージブロブを有するので、単語の境界ボックスは、単に単一の囲い込み矩形によって定義される。別の点では、我々のキャラクタリンク処理は、明示的にテキストコンポーネント間の関係を検索することに依存する他のリンクベースの方法[32,12]とは異なる画素レベルで実行される。

また、図7に示すように、文字領域全体を有効に扱うために、多角形を生成することができます。多角形生成の手順は、図7に示すように、走査方向に沿った文字領域の局所最大線を見つけることです。図7に示すように、局所最大線の長さを青色の矢印で示しています。局所最大線の長さを等しく設定して、最終的な多角形の結果が不均一にならないようにします。局所最大線のすべてを結ぶ線は、黄色で示した中心線と呼ばれます。次に、局所最大線は、赤色の矢印で示した文字の傾斜角を反映するように中心線に対して垂直に回転します。局所最大線の終点は、テキスト多角形の制御点の候補です。
![figure7](https://github.com/inouetaka/CRAFT/blob/master/images/figure7.png)

## 4.実験
### 4.1.データセット
__ICDAR2013(IC13)__ は、高解像度画像、学習画像229枚と英語テキストを含むテスト画像233枚からなる、フォーカスシーンテキスト検出のための「Robust Reading Competition for Focusted spence text」の中でリリースされ、注釈は矩形ボックスを使用した単語レベルである。

__ICDAR2015(IC15)__ は、ICDAR 2015「偶発的なシーンテキスト検出のためのロバスト・リーディング・コンクール」に導入されました。このコンクールは学習1000枚とテスト画像500枚から構成され、いずれも英語のテキストで構成されています。注釈は、4角枠を使用した単語レベルで表示されます。

__ICDAR2017(IC17)__ では、学習画像7,200枚と検証画像1,800枚、多言語シーンテキスト検出のために9言語のテキストを用いたテスト画像9,000枚が収録されており、IC15と同様に、IC17のテキスト領域にも四角形の4つの頂点が注釈付けされています。

__MSRA-TD500(TD500)__ は、ポケットカメラを用いて室内・屋外で撮影した学習画像300枚とテスト画像200枚に分割された500枚の自然画像を収録し、英語・中国語の台本を入れ、文字領域を回転矩形で注釈付けしています。

__TotalText(TotalText)__ は、ICDAR 2017で最近発表された。学習1255枚とテスト画像300を含み、特に、ポリゴンと単語レベルの転写によって注釈付けされた曲線テキストを提供します。

__CTW-1500(CTW)__ は1学習画像000枚とテスト画像500枚で構成されていますが、いずれの画像にも曲線のテキストインスタンスがあり、14の頂点を持つ多角形で注釈が付いています。
